{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the MAGI 2 database was built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes how the MAGI 2 database was built, in case you want to change something, or if an update is needed. We will describe two parts, namely how the Retro Rules database was altered to be suitable for MAGI 2 and how we obtained protein sequence information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author: Hanneke Leegwater**  \n",
    "**Last edited on: 28 July 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [0. Load python packages](#header_0)\n",
    "* [1. Build the database framework](#header_1)\n",
    "* [2. Preprocessing the Retro Rules database](#header_2)\n",
    "* [3. Precomputing reactions for all substrates](#header_3)\n",
    "* [4. Downloading protein sequences](#header_4)\n",
    "* [5. Make refseq pandas dataframe for reaction_to_gene search](#header_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load python packages <a class=\"anchor\" id=\"header_0\"></a>\n",
    "Note that not all parts were performed in Python. Also, set your path to the MAGI database here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "## you might need this if rdkit does not work\n",
    "os.environ[\"PATH\"] += r\";C:\\Users\\hanne\\Anaconda3\\envs\\magi_2\\Library\\bin\"\n",
    "import rdkit\n",
    "magi_path = \"/Users/hanne/Documents/GitHub/magi\"\n",
    "if os.path.exists(magi_path):\n",
    "    sys.path.insert(0,magi_path)\n",
    "    from workflow_2 import compound_to_reaction as magi\n",
    "    from workflow_2 import gene_to_reaction as magi_g2r\n",
    "else: \n",
    "    print(\"Path to MAGI does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the database framework <a class=\"anchor\" id=\"header_\"></a>\n",
    "Use the MAGI_database.sql file to build a database in your preferred database program and set the path here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_path = os.path.join(magi_path, \"workflow_2\", \"database\", \"MAGI_database.db\")\n",
    "database_path = \"./MAGI_database.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Retro Rules database <a class=\"anchor\" id=\"header_2\"></a>\n",
    "- We downloaded the Retro Rules preformatted database (rr02 with implicit hydrogens for RetroPath RL) from retrorules.org. We chose to use this file and not their database itself, because the preformatted tables also contain substrate SMILES, which we need for MAGI 2. The file we use is called retrorules_rr02_rp3_nohs/retrorules_rr02_flat_all.tsv. \n",
    "- We removed reactions with just H2O as a substrate and removed columns that MAGI 2 does not need. \n",
    "- Next, we calculated canonical SMILES for all substrates, which will be used to look up precomputed reactions. This took about 4 hours on a single CPU. Canonical SMILES were added to the Retro Rules dataframe in a column called Canonical_SMILES. \n",
    "- InChI keys were calculated for all canonical SMILES to speed up the lookup of compounds. These were stored in a column called InchiKey. \n",
    "- Substrates and reactions were stored in the MAGI database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_rules_path = \"./retrorules_rr02_rp3_nohs/retrorules_rr02_flat_all.tsv\"\n",
    "retro_rules = pd.read_csv(retro_rules_path, sep = \"\\t\")\n",
    "water_smiles = \"[OH2]\"\n",
    "retro_rules = retro_rules[retro_rules[\"Substrate_SMILES\"] != water_smiles]\n",
    "retro_rules = retro_rules[[\"Reaction_ID\", \"Diameter\", \"Rule_SMARTS\", \"Substrate_ID\", \"Substrate_SMILES\", \"Rule_SMILES\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substrate_smiles_all = list(set(retro_rules[\"Substrate_SMILES\"]))\n",
    "print(\"Total nr of SMILES to process is: {}\".format(len(substrate_smiles_all)))\n",
    "substrate_smiles_dict = {}\n",
    "for smiles in substrate_smiles_all:\n",
    "    canonical_smiles = magi.prepare_smiles(smiles)\n",
    "    substrate_smiles_dict[smiles] = canonical_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substrate_lookup(smiles):\n",
    "    return substrate_smiles_dict[smiles]\n",
    "retro_rules[\"Canonical_SMILES\"] = retro_rules[\"Substrate_SMILES\"].apply(substrate_lookup)\n",
    "\n",
    "substrate_objects_lookup = {}\n",
    "for smiles in list(set(retro_rules[\"Canonical_SMILES\"])):\n",
    "    substrate_objects_lookup[smiles] = magi.mol_from_smiles(smiles, useHs = False)\n",
    "def magi_mol_from_smiles_lookup(smiles):\n",
    "    return substrate_objects_lookup[smiles]\n",
    "retro_rules[\"InchiKey\"] = retro_rules[\"Substrate\"].apply(inchi.MolToInchiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_rules_substrates = retro_rules[[\"Substrate_ID\", \"Substrate_SMILES\", \"Canonical_SMILES\", \"InchiKey\"]].copy()\n",
    "retro_rules_substrates.columns = [\"retro_rules_ID\", \"retro_rules_smiles\",\"canonical_smiles\",\"inchi_key\"]\n",
    "retro_rules_substrates.drop_duplicates(inplace = True)\n",
    "retro_rules_substrates.to_sql('Retro_rules_substrates', con = connection, if_exists = \"replace\", chunksize = 1000, index_label = \"substrate_ID\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = {}\n",
    "for index, row in retro_rules_substrates.iterrows():\n",
    "    subs[row[\"Substrate_ID\"]] = index\n",
    "def substitute_substrate_ID(ID):\n",
    "    return subs[ID]\n",
    "retro_rules_reactions = retro_rules[[\"Reaction_ID\", \"Rule_SMARTS\", \"Substrate_ID\", \"Diameter\"]].copy()\n",
    "retro_rules_reactions.drop_duplicates(inplace = True)\n",
    "retro_rules_reactions.reset_index(drop = True, inplace = True)\n",
    "retro_rules_reactions.columns = [\"retro_rules_ID\", \"retro_rules_smarts\",\"substrate_ID\",\"diameter\"]\n",
    "retro_rules_reactions[\"substrate_ID\"] = retro_rules_reactions[\"substrate_ID\"].apply(substitute_substrate_ID)\n",
    "retro_rules_reactions.to_sql('Retro_rules_reactions', con = connection, if_exists = \"replace\", chunksize = 1000, index_label = \"reaction_ID\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Precomputing reactions for all substrates <a class=\"anchor\" id=\"header_3\"></a>\n",
    "This part was performed by using the The National Energy Research Scientific Computing Center facility at LBL. We subsetted all substrates from the Retro Rules table and calculated all reactions with minimum similarity 0.6 and minimum diameter 2 so that a molecule can be looked up if MAGI has calculated reactions before. We subsetted all substrates in 25 files. All scripts and input files for these MAGI runs can be found in the folder 20200131_MAGI_precomputing_reactions. However, if you update the Retro Rules reaction patterns, you will probably have to re-run the compound to reaction searches with updated substrate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_precomputed_c2r_files = \"./MAGI_precomputing_reactions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_c2r_table_total = pd.DataFrame(columns = [\"original_compound\",\"reaction_ID\",\"similarity\"])\n",
    "for nr in range(1,26):\n",
    "    filename = \"output_{}/intermediate_files/compound_to_reaction.csv\".format(str(nr))\n",
    "    print(\"Opening {} ...\".format(filename), end = \"\\t\")\n",
    "    precomputed_c2r_table = pd.read_csv(os.path.join(path_to_precomputed_c2r_files, filename))\n",
    "    precomputed_c2r_table.drop(\"compound_score\", axis = 1, inplace = True)\n",
    "    precomputed_c2r_table_total = pd.concat([precomputed_c2r_table_total, precomputed_c2r_table])\n",
    "precomputed_c2r_table_total = precomputed_c2r_table_total.drop_duplicates()\n",
    "precomputed_c2r_table_total.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_molecules = retro_rules_substrates.copy()\n",
    "precomputed_molecules[\"substrate_ID\"] = precomputed_molecules.index\n",
    "precomputed_molecules = precomputed_molecules[[\"substrate_ID\", \"retro_rules_smiles\",\"canonical_smiles\",\"inchi_key\"]] # reorder\n",
    "precomputed_molecules.columns = ['molecule_ID', 'smiles', 'canonical_smiles', 'inchi_key']\n",
    "precomputed_molecules.to_sql('Precomputed_molecules', con = connection, if_exists = 'append', chunksize = 1000, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_c2r_table_total = precomputed_c2r_table_total.merge(precomputed_molecules[[\"molecule_ID\", \"smiles\"]], how = \"left\", left_on = \"original_compound\", right_on = \"smiles\")\n",
    "precomputed_c2r_table_total.drop([\"original_compound\", \"smiles\"], axis = 1, inplace = True)\n",
    "precomputed_c2r_table_total.to_sql('Precomputed_reactions', con = connection, if_exists = 'append', chunksize = 1000, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Downloading protein sequences <a class=\"anchor\" id=\"header_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting relevant Rhea identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAGI 2's protein database is based on Rhea identifiers. First, we obtained all Rhea identifiers for which we needed sequence information from the MAGI 2 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(os.path.join(magi_path, \"workflow_2\", \"database\", \"MAGI_database.db\"))\n",
    "query = \"SELECT rhea_ID FROM Retro_rules_to_rhea_reactions\"\n",
    "rhea_IDs = pd.read_sql_query(query, connection)\n",
    "print(rhea_IDs.shape)\n",
    "print(rhea_IDs.head())\n",
    "rhea_IDs.to_csv(\"rhea_ids.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading a Rhea to UniProt table\n",
    "Rhea lists UniProt protein identifiers for all Rhea IDs. This table needs to be downloaded manually from https://www.rhea-db.org/download . The table is called rhea2uniprot.tsv and the direct link is ftp://ftp.expasy.org/databases/rhea/tsv/rhea2uniprot.tsv . We subsetted the table to only keep UniProt identifiers for reactions that are in the MAGI 2 database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhea2uniprot = pd.read_csv(\"rhea2uniprot.tsv\", sep=\"\\t\",dtype={\"RHEA_ID\":str})\n",
    "print(\"Full Rhea 2 uniprot table size is {}\".format(rhea2uniprot.shape[0]))\n",
    "print(rhea2uniprot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting relevant UniProt identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhea2uniprot = rhea2uniprot.merge(rhea_IDs, how=\"right\", left_on=\"RHEA_ID\", right_on=\"rhea_ID\")\n",
    "rhea2uniprot = rhea2uniprot[pd.notna(rhea2uniprot[\"RHEA_ID\"])][[\"rhea_ID\", \"ID\"]]\n",
    "rhea2uniprot.columns = [\"rhea_ID\", \"uniprot_ID\"]\n",
    "print(\"Subset Rhea 2 uniprot table size is {}\".format(rhea2uniprot.shape[0]))\n",
    "print(rhea2uniprot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing this to make a list of unique uniprot IDs and write these to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sequences.txt','w') as fid:\n",
    "    fid.write('\\n'.join(rhea2uniprot[\"uniprot_ID\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting UniProt sequences\n",
    "For this, go to https://www.uniprot.org/uploadlists/ and select the file of uniprot IDs. Select to search to UniProtKB identifier. Download the fasta (canonical) file from the link after it completes.  This takes about 5 minutes. Store this file as rhea2uniprot.fasta in the MAGI database folder. Run the makeblastdb script from NCBI to turn this fasta file into a database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add UniProt headers to the MAGI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_headers = {}\n",
    "with open(\"./rhea2uniprot.fasta\", \"r\") as fastafile:\n",
    "    for line in fastafile:\n",
    "        if line.startswith(\">\"):\n",
    "            ## Get ID without > symbol\n",
    "            identifier = line.split(\" \")[0][1:] \n",
    "            ## Remove first and third part from identifier\n",
    "            ## Example sp|A0PSD4|AHPD_MYCUA becomes A0PSD4\n",
    "            identifier = identifier.split(\"|\")[1] \n",
    "            uniprot_headers[identifier] = line.rstrip()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_headers = pd.DataFrame.from_dict(uniprot_headers, orient=\"index\", columns=[\"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhea2uniprot = rhea2uniprot.merge(uniprot_headers, how = \"left\", left_on = \"uniprot_ID\", right_index = True)\n",
    "def get_extended_protein_id(header):\n",
    "    protein_id = header.split(\" \")[0]\n",
    "    return protein_id\n",
    "rhea2uniprot[\"protein_ID\"] = rhea2uniprot[\"header\"].apply(get_extended_protein_id)\n",
    "rhea_to_uniprot.to_sql( 'Proteins', con = connection, if_exists = 'replace', chunksize = 1000, index = False)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build retro rules to rhea reactions table\n",
    "This is a table that links metanetx identifiers to Rhea identifiers. It was downloaded from https://www.metanetx.org/mnxdoc/mnxref.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions_info = []\n",
    "with open(\"./reac_xref.tsv\", \"r\") as metanetx_file:\n",
    "    for line in metanetx_file:\n",
    "        if line.startswith(\"rhea\"):\n",
    "            reactions_info.append(line)\n",
    "\n",
    "with open(\"./reac_xref_rhea.tsv\", \"w\") as metanetx_file:\n",
    "    for line in reactions_info:\n",
    "        metanetx_file.write(line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions_info = pd.read_csv(\"./reac_xref_rhea.tsv\", sep=\"\\t\", header = None)\n",
    "reactions_info.columns = [\"rhea_ID\", \"retro_rules_ID\", \"other\"]\n",
    "reactions_info = reactions_info[[\"retro_rules_ID\",\"rhea_ID\"]]\n",
    "reactions_info[\"rhea_ID\"] = reactions_info[\"rhea_ID\"].str.replace(\"rheaR:\",\"\")\n",
    "reactions_info = reactions_info[reactions_info['retro_rules_ID'] != \"EMPTY\"]\n",
    "reactions_info.to_sql(    'Retro_rules_to_rhea_reactions', con = connection, if_exists = 'replace', chunksize = 1000, index = False)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make refseq pandas dataframe for reaction_to_gene search <a class=\"anchor\" id=\"header_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhea_to_uniprot = pd.read_csv(\"rhea2uniprot.tsv\", sep=\"\\t\")\n",
    "rhea_to_uniprot = rhea_to_uniprot[[\"RHEA_ID\", \"ID\"]]\n",
    "rhea_to_uniprot.rename({\"RHEA_ID\": \"rhea_ID\", \"ID\": \"uniprot_ID\"}, axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_headers = {}\n",
    "with open(\"rhea2uniprot.fasta\", \"r\") as fastafile:\n",
    "    for line in fastafile:\n",
    "        if line.startswith(\">\"):\n",
    "            ## Get ID without > symbol\n",
    "            identifier = line.split(\" \")[0][1:] \n",
    "            ## Remove first and third part from identifier\n",
    "            ## Example sp|A0PSD4|AHPD_MYCUA becomes A0PSD4\n",
    "            identifier = identifier.split(\"|\")[1] \n",
    "            uniprot_headers[identifier] = line.rstrip()[1:]\n",
    "uniprot_headers = pd.DataFrame.from_dict(uniprot_headers, orient=\"index\", columns=[\"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhea_to_uniprot = rhea_to_uniprot.merge(uniprot_headers,\n",
    "                     how = \"left\",\n",
    "                     left_on = \"uniprot_ID\", right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if any headers are missing. This should be 0, otherwise you may miss sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhea_to_uniprot[pd.isna(rhea_to_uniprot[\"header\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section makes a data frame with gene IDs and gene sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_genome = magi_g2r.make_genome_dataframe_from_fasta(\"./rhea2uniprot.fasta\")\n",
    "def alter_gene_id(gene_id):\n",
    "    try:\n",
    "        gene_id = gene_id.split(\"|\")[1] \n",
    "    except:\n",
    "        pass\n",
    "    return gene_id\n",
    "reference_genome[\"Gene_ID\"] = reference_genome[\"Gene_ID\"].apply(alter_gene_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge genome sequences to the proteins that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = rhea_to_uniprot.merge(reference_genome, how=\"left\", left_on = \"uniprot_ID\", right_on = \"Gene_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicate gene sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = genome[[\"uniprot_ID\", \"sequence\"]]\n",
    "genome = genome.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to file to use for MAGI reaction to gene search and place this table in the database folder for MAGI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome.to_csv(\"./reaction_to_gene_reference.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magi_2",
   "language": "python",
   "name": "magi_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
