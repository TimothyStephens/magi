{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import StringIO\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "# reference compounds for accurate mass searching\n",
    "reference_compounds = pd.read_pickle('/Users/Onur/repos/metatlas_reactions/workflow/database/unique_compounds_groups.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'http://localhost:8000/'\n",
    "auth_url = base_url + 'admin/login'\n",
    "\n",
    "client = requests.session()\n",
    "# Retrieve the CSRF token first\n",
    "client.get(auth_url,verify=False)  # sets cookie\n",
    "username='admin' # Superuser goes here\n",
    "password='adminpass' # Superuser password goes here\n",
    "csrftoken = client.cookies['csrftoken']\n",
    "login_data = {'username': username,\n",
    "              'password': password,\n",
    "              'csrfmiddlewaretoken': csrftoken}\n",
    "client.post(auth_url, data=login_data, headers=dict(Referer=auth_url),verify=False)\n",
    "\n",
    "r = client.get(os.path.join(base_url,'admin/ids/?filter=all&json=True'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose one job for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'fields': {u'adducts_neg': u'n1',\n",
       "  u'adducts_pos': u'p1,p2,p3,p4,p8',\n",
       "  u'blast_cutoff': 99.0,\n",
       "  u'chemnet_penalty': 7.0,\n",
       "  u'email': u'oerbilgin@lbl.gov',\n",
       "  u'fasta_file': u'input/2017/08/58a7e761-2bfb-42b7-89cf-8042c1ebef9e/fasta.faa',\n",
       "  u'is_mass_search': True,\n",
       "  u'is_tautomers': False,\n",
       "  u'metabolite_file': u'input/2017/08/58a7e761-2bfb-42b7-89cf-8042c1ebef9e/metabolite.csv',\n",
       "  u'network_level': 1,\n",
       "  u'polarity': u'pos',\n",
       "  u'ppm': 10.0,\n",
       "  u'reciprocal_cutoff': 99.0,\n",
       "  u'score_weight_compound': 2.0,\n",
       "  u'score_weight_homology': 1.5,\n",
       "  u'score_weight_reciprocal': 2.5,\n",
       "  u'score_weight_rxnconnect': 0.0,\n",
       "  u'uploaded_at': u'2017-08-18T14:27:46.990Z'},\n",
       " u'model': u'magi_task.magijob',\n",
       " u'pk': u'58a7e761-2bfb-42b7-89cf-8042c1ebef9e'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_jobs = json.loads(r.text)\n",
    "job_data = all_jobs[2]\n",
    "job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make some preprocessing decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adjust filepaths\n",
    "Need to coordinate with Ben and Matt if this is necessary and what the paths will be at NERSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_root = '/project/projectdirs/metatlas/projects/magi_tasks'\n",
    "\n",
    "job_data['fields']['fasta_file'] = os.path.join(dir_root, job_data['fields']['fasta_file'].split('input/')[1])\n",
    "job_data['fields']['metabolite_file'] = os.path.join(dir_root, job_data['fields']['metabolite_file'].split('input/')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def protein_translate(seq, warnings=False):\n",
    "    \"\"\"\n",
    "    Translates an input DNA or RNA sequence to amino acid\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    seq: DNA or RNA sequence\n",
    "    warnings: print out warnings or not\n",
    "    na: nucleic acid language('dna' or 'rna')\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    protein_seq: Translated protein sequence\n",
    "    \"\"\"\n",
    "    codons = {\n",
    "'AAA': 'K', 'AAC': 'N', 'AAG': 'K', 'AAT': 'N', 'ACA': 'T', 'ACC': 'T',\n",
    "'ACG': 'T', 'ACT': 'T', 'AGA': 'R', 'AGC': 'S', 'AGG': 'R', 'AGT': 'S',\n",
    "'ATA': 'I', 'ATC': 'I', 'ATG': 'M', 'ATT': 'I', 'CAA': 'Q', 'CAC': 'H',\n",
    "'CAG': 'Q', 'CAT': 'H', 'CCA': 'P', 'CCC': 'P', 'CCG': 'P', 'CCT': 'P',\n",
    "'CGA': 'R', 'CGC': 'R', 'CGG': 'R', 'CGT': 'R', 'CTA': 'L', 'CTC': 'L',\n",
    "'CTG': 'L', 'CTT': 'L', 'GAA': 'E', 'GAC': 'D', 'GAG': 'E', 'GAT': 'D',\n",
    "'GCA': 'A', 'GCC': 'A', 'GCG': 'A', 'GCT': 'A', 'GGA': 'G', 'GGC': 'G',\n",
    "'GGG': 'G', 'GGT': 'G', 'GTA': 'V', 'GTC': 'V', 'GTG': 'V', 'GTT': 'V',\n",
    "'TAA': '*', 'TAC': 'Y', 'TAG': '*', 'TAT': 'Y', 'TCA': 'S', 'TCC': 'S',\n",
    "'TCG': 'S', 'TCT': 'S', 'TGA': '*', 'TGC': 'C', 'TGG': 'W', 'TGT': 'C',\n",
    "'TTA': 'L', 'TTC': 'F', 'TTG': 'L', 'TTT': 'F'\n",
    "        }\n",
    "    # check input sequence\n",
    "    if len(seq) % 3 != 0:\n",
    "        raise RuntimeError('The nucelotide sequence is not a multiple of 3: %s' % (seq))\n",
    "    \n",
    "    # replace all Uracil with Thymine\n",
    "    seq = seq.upper()\n",
    "    seq = seq.replace('U', 'T')\n",
    "\n",
    "    # make index tuples list\n",
    "    cidx = range(0, len(seq)+3, 3)\n",
    "    cidx = zip(cidx[:-1], cidx[1:])\n",
    "    \n",
    "    # translate\n",
    "    protein_seq = ''\n",
    "    for i, j in cidx:\n",
    "        codon = seq[i:j]\n",
    "        aa = codons[codon]\n",
    "        protein_seq += aa\n",
    "    \n",
    "    # check translation for suspicious activity\n",
    "    if warnings:\n",
    "        if protein_seq[0] != 'M':\n",
    "            print('WARNING: sequence does not start with Methionine')\n",
    "        if protein_seq[-1] != '*':\n",
    "            print('WARNING: sequence does not end with a STOP codon')\n",
    "        if '*' in protein_seq[:-1]:\n",
    "            counter = []\n",
    "            for i, aa in enumerate(protein_seq[:-1]):\n",
    "                if aa == '*':\n",
    "                    counter.append(i * 3)\n",
    "            print(\n",
    "                'WARNING: sequence contains internal STOP codons at DNA positions: %s'\n",
    "                 % (counter))\n",
    "    \n",
    "    # return the translation\n",
    "    return protein_seq\n",
    "\n",
    "def determine_fasta_language(job_data, translate=True):\n",
    "    \"\"\"\n",
    "    Assuming this is a valid fasta file,\n",
    "    determines if the fasta is protein or nucleic acid\n",
    "    if translate = True, translates the file to protein if it is dna/rna\n",
    "    \n",
    "    Input is dict of json of one job.\n",
    "    \n",
    "    alters the fasta_file field in job_data json if translation occured.\n",
    "    \n",
    "    returns job_data\n",
    "    \"\"\"\n",
    "    fil_path = job_data['fields']['fasta_file']\n",
    "    # read fasta file\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_data = f.read()\n",
    "        \n",
    "    # convert newlines\n",
    "    for newline in ['\\r\\n', '\\n\\r']:\n",
    "        if newline in file_data:\n",
    "            file_data.replace(newline, '\\n')\n",
    "    if '\\r' in file_data:\n",
    "        file_data.replace('\\r', '\\n')\n",
    "    \n",
    "    # parse gene sequences into one long string and convert to DNA if RNA\n",
    "    genes = file_data.split('>')[1:]\n",
    "    seqs = ''.join([''.join(g.split('\\n')[1:]).replace('-', '') for g in genes]).upper().replace('U', 'T')\n",
    "    \n",
    "    # determine what letters are in the gene sequences\n",
    "    letters = pd.Series(list(set(seqs)))\n",
    "    # first check for DNA\n",
    "    if len(letters) <= 4 and letters.str.contains('[ACTG]').all():\n",
    "        answer = 'dna'\n",
    "    # then amino acids \n",
    "    elif letters.str.contains('[ACDEFGHIKLMNPQRSTVWY*]').all():\n",
    "        answer = 'protein'\n",
    "    else:\n",
    "        no = letters[~letters.str.contains('[ACDEFGHIKLMNPQRSTVWY*]')].values\n",
    "        raise RuntimeError('Could not determine if FASTA is nucleotide or protein. Offending character(s): %s' % (no))\n",
    "    \n",
    "    # translate if desired\n",
    "    if translate and answer == 'dna':       \n",
    "        # translate the genes\n",
    "        new_data = ''\n",
    "        gene_list = file_data.split('>')[1:]\n",
    "        for gene in gene_list:\n",
    "            header = gene.split('\\n')[0]\n",
    "            seq = gene.split('\\n')[1:]\n",
    "            seq = ''.join([i for i in seq if i != ''])\n",
    "            protein = protein_translate(seq)\n",
    "            new_data += '>' + header + '\\n'\n",
    "            new_data += protein + '\\n\\n'\n",
    "        \n",
    "        # save the new file\n",
    "        new_filename = file_path.split('/')[-1].split('.')[0] + '_translated.faa'\n",
    "        new_filepath = '/'.join(file_path.split('/')[:-1]) + '/%s' %(new_filename)\n",
    "        with open(new_filepath, 'w') as f:\n",
    "            f.write(new_data)\n",
    "        # change the field in job_data\n",
    "        job_data['fields']['fasta_file'] = new_filepath\n",
    "        return job_data\n",
    "    else:\n",
    "        return job_data\n",
    "\n",
    "def job_script(job_data):\n",
    "    \"\"\"\n",
    "    uses job data json to create a magi job submission script for cori\n",
    "    \"\"\"\n",
    "    account_id = 'm2650' # metatlas\n",
    "    \n",
    "    # where to write the job script to\n",
    "    out_path = '/'.join(job_data['fields']['fasta_file'].split('/')[:-1])\n",
    "    \n",
    "    # need to convert this into string so we can join it later\n",
    "    job_data['fields']['score_weights'] = [str(i) for i in job_data['fields']['score_weights']]\n",
    "    \n",
    "    job_lines = [\n",
    "        '#!/bin/bash -l',\n",
    "        '#SBATCH --account=%s' % (account_id),\n",
    "        '#SBATCH --job-name=%s' % (job_data['job_id'].split('-')[0]),\n",
    "        '#SBATCH --time=0:10:00',\n",
    "        '#SBATCH --output=%s/log_out.txt' % (out_path),\n",
    "        '#SBATCH --error=%s/log_err.txt' % (out_path),\n",
    "        '#SBATCH --partition=debug',\n",
    "        '#SBATCH --constraint=haswell',\n",
    "        '#SBATCH --license=project',\n",
    "        '#SBATCH --mail-user=%s' %(job_data['fields']['email']),\n",
    "        '#SBATCH --mail-type=BEGIN,END,FAIL,TIME_LIMIT',\n",
    "        '',\n",
    "        'module load python/2.7-anaconda',\n",
    "        '',\n",
    "        'time python /project/projectdirs/metatlas/projects/metatlas_reactions/workflow/magi_workflow_20170519.py \\\\',\n",
    "        '--fasta %s \\\\' % (job_data['fields']['fasta_file']),\n",
    "        '--compounds %s \\\\' % (job_data['fields']['metabolite_file']),\n",
    "        '--level %s \\\\' % (job_data['fields']['network_level']),\n",
    "        # not sure if this line will break anything at nersc\n",
    "        # if it does, put it at the end of the previous line\n",
    "        '%s \\\\' % (['--tautomer' for i in [job_data['fields']['is_tautomers']] if i][0]),\n",
    "        '--final_weights %s \\\\' % (' '.join(job_data['fields']['score_weights'])),\n",
    "        '--blast_filter %s \\\\' % (job_data['fields']['blast_cutoff']),\n",
    "        '--reciprocal_closeness %s \\\\' % (job_data['fields']['reciprocal_cutoff']),\n",
    "        '--chemnet_penalty %s \\\\' % (job_data['fields']['chemnet_penalty']),\n",
    "        '--output %s/output_files --mute' % (out_path),\n",
    "    ]\n",
    "    \n",
    "    job = '\\n'.join(job_lines)\n",
    "    with open(os.path.join(out_path, 'job_script.sbatch'), 'w') as f:\n",
    "        f.write(job)\n",
    "    return 'job script written'\n",
    "\n",
    "def ppm_window(mass, ppm=5, result='bounds'):\n",
    "    \"\"\"\n",
    "    Given a mass and a ppm error, returns lower and upper bounds \n",
    "    corresponding to that ppm window. \n",
    "    Inputs\n",
    "    ------\n",
    "    mass:   monoisotopic mass\n",
    "    ppm:    the ppm error to construct the window\n",
    "    result: \"bounds\" returns a list of lower and upper bounds\n",
    "            \"error\" returns the amu value of the error, if you wanted to\n",
    "            add and/or subtract the error as you wish\n",
    "    Outputs\n",
    "    -------\n",
    "    Either a list of lower and upper mass bounds, or a single value\n",
    "    corresponding to the amu error\n",
    "    \"\"\"\n",
    "    error = ppm/1e6 * mass\n",
    "    lower_bound = mass - error\n",
    "    upper_bound = mass + error\n",
    "    if result.lower() == 'bounds':\n",
    "        return [lower_bound, upper_bound]\n",
    "    elif result.lower() == 'error':\n",
    "        return error\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            '%s is not a valid result argument' %(result)\n",
    "            )\n",
    "\n",
    "def accurate_mass_match(mass, compound_df=None, ppm=5, extract='inchi_key'):\n",
    "    \"\"\"\n",
    "    Accurate mass searching against a compound database.\n",
    "    Inputs\n",
    "    ------\n",
    "    mass:           An accurate monoisotopic mass\n",
    "    compound_df:    A dataframe of compounds. Must have a column named\n",
    "                    \"mono_isotopic_molecular_weight\"\n",
    "    ppm:            ppm error to allow the mass matcher\n",
    "    extract:        What compound information to return. Must correspond\n",
    "                    to a valid column in compound_df\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    cpd:            List of compounds that were matched\n",
    "    \"\"\"\n",
    "\n",
    "    err = ppm_window(mass, ppm=ppm, result='error')\n",
    "\n",
    "    potential_compounds = compound_df[\n",
    "                            abs(compound_df['mono_isotopic_molecular_weight'] \\\n",
    "                            - mass) <= err]\n",
    "    cpds = potential_compounds[extract].values.tolist()\n",
    "    if len(cpds) == 0:\n",
    "        cpds = None\n",
    "    return cpds\n",
    "\n",
    "def accurate_mass_search_wrapper(job_data):\n",
    "    \"\"\"\n",
    "    performs accurate mass search using unique_compounds table\n",
    "    \"\"\"\n",
    "\n",
    "    # math table for all adducts\n",
    "    adduct_table = {\n",
    "        'p1' : -1.007276,\n",
    "        'p2' : -18.033823,\n",
    "        'p3' : -22.989218,\n",
    "        'p4' : -38.963158,\n",
    "        'p8' : - 42.033823,\n",
    "    }\n",
    "\n",
    "    search_ppm = job_data['fields']['ppm']\n",
    "\n",
    "    # get adducts according to polarity\n",
    "    if job_data['fields']['polarity'] == 'pos':\n",
    "        adducts = job_data['fields']['adducts_pos'].split(',')\n",
    "    elif job_data['fields']['polarity'] == 'pos':\n",
    "        adducts = job_data['fields']['adducts_neg'].split(',')\n",
    "    else:\n",
    "        raise RuntimeError('Could not understand polarity')\n",
    "\n",
    "    # load compound table (should be masses in original_compounds)\n",
    "    compounds = pd.read_csv(job_data['fields']['metabolite_file'])\n",
    "\n",
    "    # rename original_compounds column\n",
    "    columns = compounds.columns.values\n",
    "    columns[columns == 'original_compound'] = 'original_mz'\n",
    "    compounds.columns = columns\n",
    "\n",
    "    # set up data container\n",
    "    data = {\n",
    "        'original_compound': [],\n",
    "        'searched_adduct': [],\n",
    "        'original_mz' : []\n",
    "    }\n",
    "    # accurate mass search and store results\n",
    "    for mz in compounds['original_mz'].unique():\n",
    "        for adduct in adducts:\n",
    "            neutral_mass = mz + adduct_table[adduct]\n",
    "            found_compounds = accurate_mass_match(neutral_mass,\n",
    "                                                  compound_df=reference_compounds,\n",
    "                                                  ppm=search_ppm\n",
    "                                                 )\n",
    "            if found_compounds is not None:\n",
    "                for cpd in found_compounds:\n",
    "                    data['original_compound'].append(cpd)\n",
    "                    data['searched_adduct'].append(adduct)\n",
    "                    data['original_mz'].append(mz)\n",
    "\n",
    "    # merge with user input and save\n",
    "    df = pd.DataFrame(data)\n",
    "    compounds = compounds.merge(df, on='original_mz', how='left')\n",
    "    \n",
    "    # save the new table\n",
    "    new_path = job_data['fields']['metabolite_file'].split('.')[0] + '_mass_searched.csv'\n",
    "    job_data['fields']['metabolite_file'] = new_path\n",
    "    compounds.to_csv(job_data['fields']['metabolite_file'])\n",
    "\n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, determine language of FASTA and translate if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna translated\n"
     ]
    }
   ],
   "source": [
    "print job_data['fields']['fasta_file']\n",
    "fasta_language = determine_fasta_language(job_data['fields']['fasta_file'])\n",
    "fasta_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, determine if accurate mass searching needs to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Onur/repos/magi_web/temp_magi_web/input/2017/08/58a7e761-2bfb-42b7-89cf-8042c1ebef9e/metabolite.csv\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'accurate mass searching done'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print job_data['fields']['metabolite_file']\n",
    "print job_data['fields']['is_mass_search']\n",
    "accurate_mass_search_wrapper(job_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last, create a job submission script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job script written\n"
     ]
    }
   ],
   "source": [
    "print job_script(job_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
